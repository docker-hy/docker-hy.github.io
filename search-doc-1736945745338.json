[{"title":"Credits","type":0,"sectionRef":"#","url":"/credits","content":"Credits The DevOps with Docker course was created by Jami Kousa with the help of University of Helsinki's Tietojenkäsittelytieteen osaston sovelluskehitysakatemia (Toska) and numerous course attendees. This material is based on gist by Matti Paksula. You can help develop the course material as well. As of 2022 the course was handed off to Matti Luukkainen the legendary creator of Full Stack Open. You can contact him on matti.luukkainen@helsinki.fi. This material is following the official Docker guidelines presented on the official website. If you find anything conflicting or otherwise prohibited use, please inform us and we'll make the required changes. This material is licenced under Creative Commons BY-NC-SA 3.0 -licence, so you can freely use and distribute the material, as long as original creators are credited. If you make changes to material and you want to distribute altered version it must be licenced under the same licence. Usage of material for commercial use is prohibited without permission.","keywords":""},{"title":"Frequently Asked Questions","type":0,"sectionRef":"#","url":"/faq","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Frequently Asked Questions","url":"/faq#prerequisites","content":"The course is suitable for anyone interested in Docker or containerization and has at least some experience with the Linux command line. It also helps if you have some experience in web development. "},{"title":"How do I sign up for the course?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/faq#how-do-i-sign-up-for-the-course","content":"There's no requirement to sign up for the course until after you've completed the exercises and wish to end your course. "},{"title":"How many credits are available?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/faq#how-many-credits-are-available","content":"The course is 1-3 ECTS credits depending on the number of completed parts. The certificate will be available through the submission application. ECTS credits are available through open university for everybody with no cost! "},{"title":"How do I submit the exercises?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/faq#how-do-i-submit-the-exercises","content":"The exercises are submitted to GitHub, and by marking the exercises as done in the exercise submission system. You can read more from getting started "},{"title":"Do I have to submit each part to a separate repository?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/faq#do-i-have-to-submit-each-part-to-a-separate-repository","content":"No. You can use one or more repositories, the most important part is to name everything in a repository clearly. "},{"title":"Can I edit a submission?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/faq#can-i-edit-a-submission","content":"No. All exercises for a part must be submitted at the same time. If you marked exercises wrong by accident or wrote the repository name wrong, send an e-mail to matti.luukkainen@helsinki.fi. "},{"title":"When and how can I get my credits?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/faq#when-and-how-can-i-get-my-credits","content":"You can get your credits after submitting enough exercises. First, complete the exercises as instructed in getting started. Then let us know through the exercise submission system that you are ready with the course. "},{"title":"Do I get a certificate after completing the course?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/faq#do-i-get-a-certificate-after-completing-the-course","content":"Yes, the certificate is available to all who pass the course before the deadline. You can download the certificate after completing the course from the exercise submission system. "},{"title":"How do I extend my 2021, 2022 or 2023 progress?​","type":1,"pageTitle":"Frequently Asked Questions","url":"/faq#how-do-i-extend-my-2021-2022-or-2023-progress","content":"You can simply submit the completed parts again. If you left 1 exercise not completed, mark one exercise not completed in the 2024 version. If you did all of the exercises mark all as done in the 2024 course. "},{"title":"Introduction to Part 1","type":0,"sectionRef":"#","url":"/part-1/","content":"Introduction to Part 1 This part introduces containerization with Docker and relevant concepts such as image and volume. By the end of this part you are able to: Run containerized applications Containerize applications Utilize volumes to store data persistently outside of the containers. Use port mapping to enable access via TCP to containerized applications Share your own containers publicly","keywords":""},{"title":"Getting Started","type":0,"sectionRef":"#","url":"/getting-started","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Getting Started","url":"/getting-started#prerequisites","content":"The course is suitable for anyone interested in Docker or containerization and has at least some experience with the Linux command line. It also helps if you have some experience in web development. "},{"title":"About different architectures and contributing​","type":1,"pageTitle":"Getting Started","url":"/getting-started#about-different-architectures-and-contributing","content":"Please note that while Docker runs on all major operating systems and even on ARM architecture, this course material may not cover platform-specific details for all operating systems. However, we've had students successfully complete the course using a variety of machines and operating systems. If you encounter any issues while working through the course material on your particular system, we recommend consulting the Docker documentation or seeking help on the course forums. Our community is here to support you and help you succeed in the course! We welcome contributions to the course material from students and other members of the DevOps community! If you notice any mistakes, typos, or errors in the material, please consider submitting a pull request to the course repository on GitHub. Thank you in advance for your contributions to this open-source project! "},{"title":"Grading​","type":1,"pageTitle":"Getting Started","url":"/getting-started#grading","content":"Passing this course requires you to do the exercises for each part. This means generally every exercise, but you are allowed to skip one non-mandatory exercise in each part. Some of the exercises are mandatory and can not be skipped. This course is worth 1-3 credits depending on the completed parts. Completing part 1 gives you 1 credit. Completing parts 1 and 2 is worth 2 credits. Completing all of the parts will grant you 3 credits. There are additional instructions for completion after each part and at the end of this page. "},{"title":"Learning objectives​","type":1,"pageTitle":"Getting Started","url":"/getting-started#learning-objectives","content":"Part 1: DevOps with Docker (TKT21036) Understand the fundamental concepts of Docker, including images and containers.Learn how to build Docker images for existing projects and run them.Understand how Docker can simplify the development process. Part 2: DevOps with Docker: docker-compose (TKT21037) Learn how to manage complex multi-container applications with Docker Compose.Understand the role of Docker Compose in container orchestrationPractice deploying and managing real-world applications using Docker Compose. Part 3: DevOps with Docker: security and optimization (TKT21038) Learn how to optimize Docker images for production, including reducing image size and improving security.Understand the limitations of using Docker Compose in production environments and the need for more advanced orchestration tools.Explore alternative container orchestration solutions, including Kubernetes. "},{"title":"Where to find information about the course?​","type":1,"pageTitle":"Getting Started","url":"/getting-started#where-to-find-information-about-the-course","content":"All of the details you need to complete the course should be found on this page. If something is missing or unclear after reading this page, please contact matti.luukkainen@helsinki.fi or get in touch through Discord. "},{"title":"Discord​","type":1,"pageTitle":"Getting Started","url":"/getting-started#discord","content":"This course has a Discord group where we discuss everything about the course. Support is available almost 24/7, with the discussion being in both English and Finnish. Join our discord group here. All inappropriate, degrading or discriminating comments on the channel are prohibited and will lead to action taken against the commenter. Warning: Before installing Docker Containers leverage the power of your own operating system. As such by default any containerized application, or user who has external access to your container, would have super user privileges to your computer. I will try my best to alert you of potential risks as we encounter them, but due to the structure of the course we will focus on security in part 3. Please keep this in mind as you move through the installation and exercises. If you ever feel unsure about what you're doing, come to the Discord channel and have a chat with us. "},{"title":"Installing Docker​","type":1,"pageTitle":"Getting Started","url":"/getting-started#installing-docker","content":"Use the official documentation to find download instructions for docker-ce for the platform of your choice: Ubuntu If you have fuksiläppäri, that is, a freshman laptop of University of Helsinki, do the installation as describedhere MacOS Windows Confirm that Docker installed correctly by opening a terminal and running docker -v to see the installed version. Docker group To avoid writing sudos you may consider adding yourself to docker group Keep in mind that if you do so, you can now run containers without sudo and containers give you super user access to the computer. "},{"title":"Deadline​","type":1,"pageTitle":"Getting Started","url":"/getting-started#deadline","content":"The sign up for ECTS credits and the course ends 16.6.2024! After that course is locked and submissions can no longer be made or credits earned. As the certificate is received through submissions, you have to submit everything before the course ends. More details under completion and after each part. "},{"title":"General guidance​","type":1,"pageTitle":"Getting Started","url":"/getting-started#general-guidance","content":"Do not alter the code of the projects, unless by pull-requests to the original projects You do not need to touch Ruby, Java, Javascript or Python code during this course. You may have to read their error messages. Visit the Discord channel if you are stuck! "},{"title":"How to submit the exercises​","type":1,"pageTitle":"Getting Started","url":"/getting-started#how-to-submit-the-exercises","content":"Make a repository to GitHub and publish your solutions in clearly ordered files / folders. If you need help publishing using Git you should refer to their guide. Make sure that the repository is available to us, either by using a public repository or a private repository and adding Jakousa and mluukkai as collaborators. Most of the exercises will be focused on a Dockerfile and/or docker-compose.yml. In those cases, submitting the file is enough. In other cases, a picture or copy-paste from your command line or a link to Docker Hub and/or project inside the repository is enough. For the command line exercises at start the command script may be helpful. Because the course exercises are designed to build upon each other, it's more important that you document the exercises for yourself. We will be looking at the submissions of the later exercises as they are more demanding. When you have completed a part, use the submission application to mark your exercises. You can not edit a submission, so make sure you have completed enough exercises for a part before submitting. "},{"title":"Completing​","type":1,"pageTitle":"Getting Started","url":"/getting-started#completing","content":"The certificate is available from the small icon beneath your submissions! After you have returned all of the required exercises and wish to end your course completion and want the ECTS credits press the following button in the submission application (ignore the message about an &quot;exam&quot; as there is no exam in this course):  After that, double-check that the application has the message &quot;Course marked as completed&quot; and the date. If the date is not visible, we have not been notified.  "},{"title":"Definitions and basic concepts","type":0,"sectionRef":"#","url":"/part-1/section-1","content":"","keywords":""},{"title":"What is DevOps?​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#what-is-devops","content":"Before we get started with Docker let's lay the groundwork for learning the right mindset. Defining DevOps is not a trivial task but the term itself consists of two parts, Dev and Ops. Dev refers to the development of software and Ops to operations. Simple definition for DevOps would be that it means the release, configuring, and monitoring of software is in the hands of the very people who develop it. A more formal definition is offered by Jabbari et al.: &quot;DevOps is a development methodology aimed at bridging the gap between Development and Operations, emphasizing communication and collaboration, continuous integration, quality assurance and delivery with automated deployment utilizing a set of development practices&quot;.  Image of DevOps toolchain by Kharnagy from wikipedia Sometimes DevOps is regarded as a role that one person or a team can fill. Here's some external motivation to learn DevOps skills: Salary by Developer Type in StackOverflow survey. You will not become a DevOps specialist solely from this course, but you will get the skills to help you navigate in the increasingly containerized world. During this course we will focus mainly on the packaging, releasing and configuring of the applications. You will not be asked to plan or create new software. We will go over Docker and a few technologies that you may see in your daily life, these include e.g. Redis and Postgres. See StackOverflow survey on how closely they correlate these technologies. "},{"title":"What is Docker?​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#what-is-docker","content":"&quot;Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers.&quot; - from Wikipedia. So stripping the jargon we get two definitions: Docker is a set of tools to deliver software in containers.Containers are packages of software.  The above image illustrates how containers include the application and its dependencies. These containers are isolated so that they don't interfere with each other or the software running outside of the containers. In case you need to interact with them or enable interactions between them, Docker offers tools to do so. "},{"title":"Benefits from containers​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#benefits-from-containers","content":"Containers package applications. Sounds simple, right? To illustrate the potential benefits let's talk about different scenarios. "},{"title":"Scenario 1: Works on my machine​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#scenario-1-works-on-my-machine","content":"Let's first take a closer look into what happens in web development without containers following the chain above starting from &quot;Plan&quot;. First you plan an application. Then your team of 1-n developers create the software. It works on your computer. It may even go through a testing pipeline working perfectly. You send it to the server and... ...it does not work. This is known as the &quot;works on my machine&quot; problem. The only way to solve this is by finding out what in tarnation the developer had installed on their machine that made the application work. Containers solve this problem by allowing the developer to personally run the application inside a container, which then includes all of the dependencies required for the app to work. You may still occasionally hear about &quot;works in my container&quot; issues - these are often just usage errors. "},{"title":"Scenario 2: Isolated environments​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#scenario-2-isolated-environments","content":"You have 5 different Python applications. You need to deploy them to a server that already has an application requiring Python 2.7 and of course none of your applications are 2.7. What do you do? Since containers package the software with all of its dependencies, you package the existing app and all 5 new ones with their respective Python versions and that's it. I can only imagine the disaster that would result if you try to run them side by side on the same machine without isolating the environments. It sounds more like a time bomb. Sometimes different parts of a system may change over time, possibly leading to the application not working. These changes may be anything from an operating system update to changes in dependencies. "},{"title":"Scenario 3: Development​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#scenario-3-development","content":"You are brought into a dev team. They run a web app that uses other services when running: a Postgres database, MongoDB, Redis and a number of others. Simple enough, you install whatever is required to run the application and all of the applications that it depends on... What a headache to start installing and then managing the development databases on your own machine. Thankfully, by the time you are told to do that you are already a Docker expert. With one command you get an isolated application, like Postgres or Mongo, running in your machine. "},{"title":"Scenario 4: Scaling​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#scenario-4-scaling","content":"Starting and stopping a Docker container has little overhead. But when you run your own Netflix or Facebook, you want to meet the changing demand. With some advanced tooling that we will learn about in parts 2 and 3, we can spin up multiple containers instantly and load balance traffic between them. Container orchestration will be discussed in parts 2 and 3. But the simplest example: what happens when one application dies? The orchestration system notices it, splits traffic between the working replicas, and spins up a new container to replace the dead one. "},{"title":"Virtual machines​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#virtual-machines","content":"Isn't there already a solution for this? Virtual Machines are not the same as Containers - they solve different problems. We will not be looking into Virtual Machines in this course. However, here's a diagram to give you a rough idea of the difference.  The difference between a virtual machine and Docker solutions arises after moving Application A to an incompatible system &quot;Operating System B&quot;. Running software on top of containers is almost as efficient as running it &quot;natively&quot; outside containers, at least when compared to virtual machines. So containers have a direct access to your own Operating Systems kernel and resources. The resource usage overhead of using containers is minimized, as the applications behave as if there were no extra layers. As Docker is using Linux kernels, Mac and Windows can't run it without a few hoops and each have their own solutions on how to run Docker. "},{"title":"Running containers​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#running-containers","content":"You already have Docker installed so let's run our first container! The hello-world is a simple application that outputs &quot;Hello from Docker!&quot; and some additional info. Simply run docker container run hello-world, the output will be the following: $ docker container run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world b8dfde127a29: Pull complete Digest: sha256:308866a43596e83578c7dfa15e27a73011bdd402185a84c5cd7f32a88b501a24 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/  The command docker container run has a shorthand form docker run, so instead of $ docker container run hello-world  we could do the same with less typing and use just $ docker run hello-world  If you already ran hello-world previously it will skip the first 5 lines. The first 5 lines tell that an image &quot;hello-world:latest&quot; wasn't found and it was downloaded. Try it again: $ docker run hello-world Hello from Docker! ...  It found the image locally so it skipped right to running the hello-world. Security reminder: Downloading from the internet Keep in mind that we are downloading stuff from the internet. Double checking what you're running is always a good idea. So that's an image? "},{"title":"Image and containers​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#image-and-containers","content":"Since we already know what containers are it's easier to explain images through them: Containers are instances of images. A basic mistake is to confuse images and containers. Cooking metaphor: Think of a container as a ready-to-eat meal that you can simply heat up and consume. An image, on the other hand, is the recipe or ingredients for that meal. So just like how you need a recipe and ingredients to make a meal, you need an image and a container runtime (Docker engine) to create a container. The image provides all the necessary instructions and dependencies for the container to run, just like a recipe provides the steps and ingredients to make a meal. In short, an image is like a blueprint or template, while a container is an instance of that blueprint or template. "},{"title":"Image​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#image","content":"A Docker image is a file. An image never changes; you can not edit an existing file. Creating a new image happens by starting from a base image and adding new layers to it. We will talk about layers later, but you should think of images as immutable, they can not be changed after they are created. List all your images with docker image ls $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest d1165f221234 9 days ago 13.3kB  Containers are created from images, so when we ran hello-world twice we downloaded one image and created two of them from the single image. Well then, if images are used to create containers, where do images come from? This image file is built from an instructional file named Dockerfile that is parsed when you run docker image build. Dockerfile is a file that is by default called Dockerfile, that looks something like this Dockerfile FROM &lt;image&gt;:&lt;tag&gt; RUN &lt;install some dependencies&gt; CMD &lt;command that is executed on `docker container run`&gt;  and is the instruction set for building an image. We will look into Dockerfiles later when we get to build our own image. If we go back to the cooking metaphor, as Dockerfile provides the instructions needed to build an image you can think of that as the recipe for images. We're now 2 recipes deep, as Dockerfile is the recipe for an image and an image is the recipe for a container. The only difference is that Dockerfile is written by us, whereas image is written by our machine based on the Dockerfile! "},{"title":"Container​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#container","content":"Containers only contain what is required to execute an application; and you can start, stop and interact with them. They are isolated environments in the host machine with the ability to interact with each other and the host machine itself via defined methods (TCP/UDP). List all your containers with docker container ls $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES  Without -a flag it will only print running containers. The hello-worlds we ran already exited. $ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b7a53260b513 hello-world &quot;/hello&quot; 5 minutes ago Exited (0) 5 minutes ago brave_bhabha 1cd4cb01482d hello-world &quot;/hello&quot; 8 minutes ago Exited (0) 8 minutes ago vibrant_bell  The command docker container ls has also a shorter form docker ps that is preferred by many since it requires much less typing... "},{"title":"Docker CLI basics​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#docker-cli-basics","content":"We are using the command line to interact with the &quot;Docker Engine&quot; that is made up of 3 parts: CLI, a REST API and Docker daemon. When you run a command, e.g. docker container run, behind the scenes the client sends a request through the REST API to the Docker daemon which takes care of images, containers and other resources. You can read the docs for more information. But even though you will find over 50 commands in the documentation, only a handful of them is needed for general use. There's a list of the most commonly used basic commands at the end of this section. One of them is already familiar: docker container run &lt;image&gt;, which instructs daemon to create a container from the image and downloading the image if it is not available. Let's remove the image since we will not need it anymore, docker image rm hello-world sounds about right. However, this should fail with the following error: $ docker image rm hello-world Error response from daemon: conflict: unable to remove repository reference &quot;hello-world&quot; (must force) - container &lt;container ID&gt; is using its referenced image &lt;image ID&gt;  This means that a container that was created from the image hello-world still exists and that removing hello-world could have consequences. So before removing images, you should have the referencing container removed first. Forcing is usually a bad idea, especially as we are still learning. Run docker container ls -a to list all containers again. $ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b7a53260b513 hello-world &quot;/hello&quot; 35 minutes ago Exited (0) 35 minutes ago brave_bhabha 1cd4cb01482d hello-world &quot;/hello&quot; 41 minutes ago Exited (0) 41 minutes ago vibrant_bell  Notice that containers have a CONTAINER ID and NAME. The names are currently autogenerated. When we have a lot of different containers, we can use grep (or another similar utility) to filter the list: $ docker container ls -a | grep hello-world  Let's remove the container with docker container rm command. It accepts a container's name or ID as its arguments. Notice that the command also works with the first few characters of an ID. For example, if a container's ID is 3d4bab29dd67, you can use docker container rm 3d to delete it. Using the shorthand for the ID will not delete multiple containers, so if you have two IDs starting with 3d, a warning will be printed, and neither will be deleted. You can also use multiple arguments: docker container rm id1 id2 id3 If you have hundreds of stopped containers and you wish to delete them all, you should use docker container prune. Prune can also be used to remove &quot;dangling&quot; images with docker image prune. Dangling images are images that do not have a name and are not used. They can be created manually and are automatically generated during build. Removing them just saves some space. And finally you can use docker system prune to clear almost everything. We aren't yet familiar with the exceptions that docker system prune does not remove. After removing all of the hello-world containers, run docker image rm hello-world to delete the image. You can use docker image ls to confirm that the image is not listed. You can also use the image pull command to download images without running them: docker image pull hello-world Let's try starting a new container: $ docker run nginx  With some containers the command line appears to freeze after pulling and starting the container. This might be because that particular container is now running in the current terminal, blocking the input. You can observe this with docker container ls from another terminal. In this situation one can exit by pressing control + c and try again with the -d flag. $ docker run -d nginx c7749cf989f61353c1d433466d9ed6c45458291106e8131391af972c287fb0e5  The -d flag starts a container detached, meaning that it runs in the background. The container can be seen with $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c7749cf989f6 nginx &quot;nginx -g 'daemon of…&quot; 35 seconds ago Up 34 seconds 80/tcp blissful_wright  Now if we try to remove it, it will fail: $ docker container rm blissful_wright Error response from daemon: You cannot remove a running container c7749cf989f61353c1d433466d9ed6c45458291106e8131391af972c287fb0e5. Stop the container before attempting removal or force remove  We should first stop the container using docker container stop blissful_wright, and then use rm. Forcing is also a possibility and we can use docker container rm --force blissful_wright safely in this case. Again for both of them instead of name we could have used the ID or parts of it, e.g. c77. It's common for the Docker daemon to become clogged over time with old images and containers. "},{"title":"Most used commands​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#most-used-commands","content":"command\texplain\tshorthanddocker image ls\tLists all images\tdocker images docker image rm &lt;image&gt;\tRemoves an image\tdocker rmi docker image pull &lt;image&gt;\tPulls image from a docker registry\tdocker pull docker container ls -a\tLists all containers\tdocker ps -a docker container run &lt;image&gt;\tRuns a container from an image\tdocker run docker container rm &lt;container&gt;\tRemoves a container\tdocker rm docker container stop &lt;container&gt;\tStops a container\tdocker stop docker container exec &lt;container&gt;\tExecutes a command inside the container docker exec For all of them container can be either the container id or the container name. Same for images. In the future we may use the shorthands in the material. Some of the shorthands are legacy version of doing the same thing. You can use either. "},{"title":"Exercises 1.1-1.2​","type":1,"pageTitle":"Definitions and basic concepts","url":"/part-1/section-1#exercises-11-12","content":"Exercise 1.1: Getting started Since we already did &quot;Hello, World!&quot; in the material let's do something else. Start 3 containers from an image that does not automatically exit (such as nginx) in detached mode. Stop two of the containers and leave one container running. Submit the output for docker ps -a which shows 2 stopped containers and one running. Exercise 1.2: Cleanup We have containers and an image that are no longer in use and are taking up space. Running docker ps -a and docker image ls will confirm this. Clean the Docker daemon by removing all images and containers. Submit the output for docker ps -a and docker image ls "},{"title":"Running and stopping containers","type":0,"sectionRef":"#","url":"/part-1/section-2","content":"","keywords":""},{"title":"Running processes inside a container with docker exec​","type":1,"pageTitle":"Running and stopping containers","url":"/part-1/section-2#running-processes-inside-a-container-with-docker-exec","content":"We often encounter situations where we need to execute commands within a running container. This can be achieved using the docker exec command. We could e.g. list all the files inside the container default directory (which is the root) as follows: $ docker exec looper ls -la total 56 drwxr-xr-x 1 root root 4096 Mar 6 10:24 . drwxr-xr-x 1 root root 4096 Mar 6 10:24 .. -rwxr-xr-x 1 root root 0 Mar 6 10:24 .dockerenv lrwxrwxrwx 1 root root 7 Feb 27 16:01 bin -&gt; usr/bin drwxr-xr-x 2 root root 4096 Apr 18 2022 boot drwxr-xr-x 5 root root 360 Mar 6 10:24 dev drwxr-xr-x 1 root root 4096 Mar 6 10:24 etc drwxr-xr-x 2 root root 4096 Apr 18 2022 home lrwxrwxrwx 1 root root 7 Feb 27 16:01 lib -&gt; usr/lib drwxr-xr-x 2 root root 4096 Feb 27 16:01 media drwxr-xr-x 2 root root 4096 Feb 27 16:01 mnt drwxr-xr-x 2 root root 4096 Feb 27 16:01 opt dr-xr-xr-x 293 root root 0 Mar 6 10:24 proc drwx------ 2 root root 4096 Feb 27 16:08 root drwxr-xr-x 5 root root 4096 Feb 27 16:08 run lrwxrwxrwx 1 root root 8 Feb 27 16:01 sbin -&gt; usr/sbin drwxr-xr-x 2 root root 4096 Feb 27 16:01 srv dr-xr-xr-x 13 root root 0 Mar 6 10:24 sys drwxrwxrwt 2 root root 4096 Feb 27 16:08 tmp drwxr-xr-x 11 root root 4096 Feb 27 16:01 usr drwxr-xr-x 11 root root 4096 Feb 27 16:08 var  We can execute the Bash shell in the container in interactive mode and then run any commands within that Bash session: $ docker exec -it looper bash root@2a49df3ba735:/# ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.0 2612 1512 pts/0 Ss+ 12:36 0:00 sh -c while true; do date; sleep 1; done root 64 1.5 0.0 4112 3460 pts/1 Ss 12:36 0:00 bash root 79 0.0 0.0 2512 584 pts/0 S+ 12:36 0:00 sleep 1 root 80 0.0 0.0 5900 2844 pts/1 R+ 12:36 0:00 ps aux  From the ps aux listing we can see that our bash process got PID (process ID) of 64. Now that we're inside the container it behaves as you'd expect from Ubuntu, and we can exit the container with exit and then either kill or stop the container. Our looper won't stop for a SIGTERM signal sent by a stop command. To terminate the process, stop follows the SIGTERM with a SIGKILL after a grace period. In this case, it's simply faster to use kill. $ docker kill looper $ docker rm looper  Running the previous two commands is basically equivalent to running docker rm --force looper Let's start another process with -it and add --rm in order to remove it automatically after it has exited. The --rm ensures that there are no garbage containers left behind. It also means that docker start can not be used to start the container after it has exited. $ docker run -d --rm -it --name looper-it ubuntu sh -c 'while true; do date; sleep 1; done'  Now let's attach to the container and hit control+p, control+q to detach us from the STDOUT. $ docker attach looper-it Mon Jan 15 19:50:42 UTC 2018 Mon Jan 15 19:50:43 UTC 2018 ^P^Qread escape sequence  Instead, if we had used ctrl+c, it would have sent a kill signal followed by removing the container as we specified --rm in docker run command. "},{"title":"Exercise 1.3​","type":1,"pageTitle":"Running and stopping containers","url":"/part-1/section-2#exercise-13","content":"Exercise 1.3: Secret message Now that we've warmed up it's time to get inside a container while it's running! Image devopsdockeruh/simple-web-service:ubuntu will start a container that outputs logs into a file. Go inside the running container and use tail -f ./text.log to follow the logs. Every 10 seconds the clock will send you a &quot;secret message&quot;. Submit the secret message and command(s) given as your answer. "},{"title":"Nonmatching host platform​","type":1,"pageTitle":"Running and stopping containers","url":"/part-1/section-2#nonmatching-host-platform","content":"If you are working with M1/M2 Mac, you quite likely end up with the following warning when running the image devopsdockeruh/simple-web-service:ubuntu: WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested  Despite this warning, you can run the container. The warning basically says what's wrong, the image uses a different processor architecture than your machine. The image can be used because Docker Desktop for Mac employs an emulator by default when the image's processor architecture does not match the host's. However, it's important to note that emulated execution may be less efficient in terms of performance than running the image on a compatible native processor architecture. When you run docker run ubuntu for example, you don't get a warning, why is that? Quite a few popular images are so-called multi platform images, which means that one image contains variations for different architectures. When you are about to pull or run such an image, Docker will detect the host architecture and give you the correct type of image. "},{"title":"Ubuntu in a container is just... Ubuntu​","type":1,"pageTitle":"Running and stopping containers","url":"/part-1/section-2#ubuntu-in-a-container-is-just-ubuntu","content":"A container that is running a Ubuntu image works quite like a normal Ubuntu: $ docker run -it ubuntu root@881a1d4ecff2:/# ls bin dev home media opt root sbin sys usr boot etc lib mnt proc run srv tmp var root@881a1d4ecff2:/# ps PID TTY TIME CMD 1 pts/0 00:00:00 bash 13 pts/0 00:00:00 ps root@881a1d4ecff2:/# date Wed Mar 1 12:08:24 UTC 2023 root@881a1d4ecff2:/#  An image like Ubuntu contains already a nice set of tools but sometimes just the one that we need is not within the standard distribution. Let us assume that we would like to edit some files inside the container. The good old Nano editor is a perfect fit for our purposes. We can install it in the container by using apt-get: $ docker run -it ubuntu root@881a1d4ecff2:/# apt-get update root@881a1d4ecff2:/# apt-get -y install nano root@881a1d4ecff2:/# cd tmp/ root@881a1d4ecff2:/tmp# nano temp_file.txt  As can be seen, installing a program or library to a container happens just like the installation is done in &quot;normal&quot; Ubuntu. The remarkable difference is that the installation of Nano is not permanent, that is, if we remove our container, all is gone. We shall soon see how to get a more permanent solution for building images that are perfect to our purposes. "},{"title":"Exercise 1.4​","type":1,"pageTitle":"Running and stopping containers","url":"/part-1/section-2#exercise-14","content":"Exercise 1.4: Missing dependencies Start a Ubuntu image with the process sh -c 'while true; do echo &quot;Input website:&quot;; read website; echo &quot;Searching..&quot;; sleep 1; curl http://$website; done' If you're on Windows, you'll want to switch the ' and &quot; around: sh -c &quot;while true; do echo 'Input website:'; read website; echo 'Searching..'; sleep 1; curl http://$website; done&quot;. You will notice that a few things required for proper execution are missing. Be sure to remind yourself which flags to use so that the container actually waits for input. Note also that curl is NOT installed in the container yet. You will have to install it from inside of the container. Test inputting helsinki.fi into the application. It should respond with something like &lt;html&gt; &lt;head&gt; &lt;title&gt;301 Moved Permanently&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Moved Permanently&lt;/h1&gt; &lt;p&gt;The document has moved &lt;a href=&quot;http://www.helsinki.fi/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; This time return the command you used to start process and the command(s) you used to fix the ensuing problems. Hint for installing the missing dependencies you could start a new process with docker exec. This exercise has multiple solutions, if the curl for helsinki.fi works then it's done. Can you figure out other (smart) solutions? "},{"title":"Interacting with the container via volumes and ports","type":0,"sectionRef":"#","url":"/part-1/section-5","content":"","keywords":""},{"title":"Exercise 1.9​","type":1,"pageTitle":"Interacting with the container via volumes and ports","url":"/part-1/section-5#exercise-19","content":"Exercise 1.9: Volumes In this exercise we won't create a new Dockerfile. Image devopsdockeruh/simple-web-service creates a timestamp every two seconds to /usr/src/app/text.log when it's not given a command. Start the container with a bind mount so that the logs are created into your filesystem. Submit the command you used to complete the exercise. Hint: read the note that was made just before this exercise! "},{"title":"Allowing external connections into containers​","type":1,"pageTitle":"Interacting with the container via volumes and ports","url":"/part-1/section-5#allowing-external-connections-into-containers","content":"This course does not provide an in-depth exploration of inter-program communication mechanisms. If you want to learn that in-depth, you should look at classes about Operating Systems or Networking. Here, you just need to know a few simple things: Sending messages: Programs can send messages to URL addresses such as this: http://127.0.0.1:3000 where HTTP is the protocol, 127.0.0.1 is an IP address, and 3000 is a port. Note the IP part could also be a hostname: 127.0.0.1 is also called localhost so instead you could use http://localhost:3000. Receiving messages: Programs can be assigned to listen to any available port. If a program is listening for traffic on port 3000, and a message is sent to that port, the program will receive and possibly process it. The address 127.0.0.1 and hostname localhost are special ones, they refer to the machine or container itself, so if you are on a container and send a message to localhost, the target is the same container. Similarly, if you are sending the request from outside of a container to localhost, the target is your machine. It is possible to map your host machine port to a container port. For example, if you map port 1000 on your host machine to port 2000 in the container, and then you send a message to http://localhost:1000 on your computer, the container will get that message if it's listening to its port 2000. Opening a connection from the outside world to a Docker container happens in two steps: Exposing port Publishing port Exposing a container port means telling Docker that the container listens to a certain port. This doesn't do much, except it helps humans with the configuration. Publishing a port means that Docker will map host ports to the container ports. To expose a port, add the line EXPOSE &lt;port&gt; in your Dockerfile To publish a port, run the container with -p &lt;host-port&gt;:&lt;container-port&gt; If you leave out the host port and only specify the container port, Docker will automatically choose a free port as the host port: $ docker run -p 4567 app-in-port  We could also limit connections to a certain protocol only, e.g. UDP by adding the protocol at the end: EXPOSE &lt;port&gt;/udp and -p &lt;host-port&gt;:&lt;container-port&gt;/udp. Security reminder: Opening a door to the internet Since we are opening a port to the application, anyone from the internet could come in and access what you're running. Don't haphazardly open just any ports - a way for an attacker to get in is by exploiting a port you opened to an insecure server. An easy way to avoid this is by defining the host-side port like this -p 127.0.0.1:3456:3000. This will only allow requests from your computer through port 3456 to the application port 3000, with no outside access allowed. The short syntax, -p 3456:3000, will result in the same as -p 0.0.0.0:3456:3000, which truly is opening the port to everyone. Usually, this isn't risky. But depending on the application, it is something you should consider! "},{"title":"Exercise 1.10​","type":1,"pageTitle":"Interacting with the container via volumes and ports","url":"/part-1/section-5#exercise-110","content":"Exercise 1.10: Ports open In this exercise, we won't create a new Dockerfile. The image devopsdockeruh/simple-web-service will start a web service in port 8080 when given the argument &quot;server&quot;. In Exercise 1.8 you already did an image that can be used to run the web service without any argument. Use now the -p flag to access the contents with your browser. The output to your browser should be something like:{ message: &quot;You connected to the following path: ... Submit your used commands for this exercise. "},{"title":"In-depth dive into images","type":0,"sectionRef":"#","url":"/part-1/section-3","content":"","keywords":""},{"title":"Where do the images come from?​","type":1,"pageTitle":"In-depth dive into images","url":"/part-1/section-3#where-do-the-images-come-from","content":"When running a command such as docker run hello-world, Docker will automatically search Docker Hub for the image if it is not found locally. This means that we can pull and run any public image from Docker's servers. For example‚ if we wanted to start an instance of the PostgreSQL database, we could just run docker run postgres, which would pull and run https://hub.docker.com/_/postgres/. We can search for images in the Docker Hub with docker search. Try running docker search hello-world. The search finds plenty of results, and prints each image's name, short description, amount of stars, and &quot;official&quot; and &quot;automated&quot; statuses. $ docker search hello-world NAME DESCRIPTION STARS OFFICIAL AUTOMATED hello-world Hello World!… 1988 [OK] kitematic/hello-world-nginx A light-weig… 153 tutum/hello-world Image to tes… 90 [OK] ...  Let's examine the list. The first result, hello-world, is an official image. Official images are curated and reviewed by Docker, Inc. and are usually actively maintained by the authors. They are built from repositories in the docker-library. When browsing the CLI's search results, you can recognize an official image from the &quot;[OK]&quot; in the &quot;OFFICIAL&quot; column and also from the fact that the image's name has no prefix (aka organization/user). When browsing Docker Hub, the page will show &quot;Docker Official Images&quot; as the repository, instead of a user or organization. For example, see the Docker Hub page of the hello-world image. The third result, tutum/hello-world, is marked as &quot;automated&quot;. This means that the image is automatically built from the source repository. Its Docker Hub page shows its previous &quot;Builds&quot; and a link to the image's &quot;Source Repository&quot; (in this case, to GitHub) from which Docker Hub builds the image. The second result, kitematic/hello-world-nginx, is neither an official nor an automated image. We can't know what the image is built from, since its Docker Hub page has no links to any repositories. The only thing its Docker Hub page reveals is that the image is 9 years old. Even if the image's &quot;Overview&quot; section had links to a repository, we would have no guarantees that the published image was built from that source. There are also other Docker registries competing with Docker Hub, such as Quay. By default, docker search will only search from Docker Hub, but to search a different registry, you can add the registry address before the search term, for example, docker search quay.io/hello. Alternatively, you can use the registry's web pages to search for images. Take a look at the page of the nordstrom/hello-world image on Quay. The page shows the command to use to pull the image, which reveals that we can also pull images from hosts other than Docker Hub: docker pull quay.io/nordstrom/hello-world So, if the host's name (here: quay.io) is omitted, it will pull from Docker Hub by default. NOTE: Trying the above command may fail giving manifest errors as the default tag latest is not present in quay.io/nordstrom/hello-world image. Specifying a correct tag for a image will pull the image without any errors, for ex.docker pull quay.io/nordstrom/hello-world:2.0 "},{"title":"A detailed look into an image​","type":1,"pageTitle":"In-depth dive into images","url":"/part-1/section-3#a-detailed-look-into-an-image","content":"Let's go back to a more relevant image than 'hello-world', the Ubuntu image, one of the most common Docker images to use as a base for your own image. Let's pull Ubuntu and look at the first lines: $ docker pull ubuntu Using default tag: latest latest: Pulling from library/ubuntu  Since we didn't specify a tag, Docker defaulted to latest, which is usually the latest image built and pushed to the registry. However, in this case, the repository's README says that the ubuntu:latest tag points to the &quot;latest LTS&quot; instead since that's the version recommended for general use. Images can be tagged to save different versions of the same image. You define an image's tag by adding :&lt;tag&gt; after the image's name. Ubuntu's Docker Hub page reveals that there's a tag named 22.04 which promises us that the image is based on Ubuntu 22.04. Let's pull that as well: $ docker pull ubuntu:22.04 22.04: Pulling from library/ubuntu c2ca09a1934b: Downloading [============================================&gt; ] 34.25MB/38.64MB d6c3619d2153: Download complete 0efe07335a04: Download complete 6b1bb01b3a3b: Download complete 43a98c187399: Download complete  Images are composed of different layers that are downloaded in parallel to speed up the download. Images being made of layers also have other aspects and we will talk about them in part 3. We can also tag images locally for convenience, for example, docker tag ubuntu:22.04 ubuntu:jammy_jellyfish creates the tag ubuntu:jammy_jellyfish which refers to ubuntu:22.04. Tagging is also a way to &quot;rename&quot; images. Run docker tag ubuntu:22.04 fav_distro:jammy_jellyfish and check docker image ls to see what effects the command had. To summarize, an image name may consist of 3 parts plus a tag. Usually like the following: registry/organisation/image:tag. But may be as short as ubuntu, then the registry will default to Docker hub, organisation to library and tag to latest. The organisation may also be a user, but calling it an organisation may be more clear. "},{"title":"Exercises 1.5 - 1.6​","type":1,"pageTitle":"In-depth dive into images","url":"/part-1/section-3#exercises-15---16","content":"Exercise 1.5: Sizes of images In the Exercise 1.3 we used devopsdockeruh/simple-web-service:ubuntu. Here is the same application but instead of Ubuntu is using Alpine Linux: devopsdockeruh/simple-web-service:alpine. Pull both images and compare the image sizes. Go inside the Alpine container and make sure the secret message functionality is the same. Alpine version doesn't have bash but it has sh, a more bare-bones shell. Exercise 1.6: Hello Docker Hub Run docker run -it devopsdockeruh/pull_exercise. The command will wait for your input. Navigate through the Docker hub to find the docs and Dockerfile that was used to create the image. Read the Dockerfile and/or docs to learn what input will get the application to answer a &quot;secret message&quot;. Submit the secret message and command(s) given to get it as your answer. "},{"title":"Building images​","type":1,"pageTitle":"In-depth dive into images","url":"/part-1/section-3#building-images","content":"Finally, we get to build our own images and get to talk about Dockerfile and why it's so great. Dockerfile is simply a file that contains the build instructions for an image. You define what should be included in the image with different instructions. We'll learn about the best practices here by creating one. Let's take a most simple application and containerize it first. Here is a script called &quot;hello.sh&quot; hello.sh #!/bin/sh echo &quot;Hello, docker!&quot;  First, we will test that it even works. Create the file, add execution permissions and run it: $ chmod +x hello.sh $ ./hello.sh Hello, docker!  If you're using Windows you can skip these two and add chmod +x hello.sh to the Dockerfile. And now to create an image from it. We'll have to create the Dockerfile that declares all of the required dependencies. At least it depends on something that can run shell scripts. We will choose Alpine, a small Linux distribution that is often used to create small images. Even though we're using Alpine here, you can use Ubuntu during exercises. Ubuntu images by default contain more tools to debug what is wrong when something doesn't work. In part 3 we will talk more about why small images are important. We will choose exactly which version of a given image we want to use. This guarantees that we don't accidentally update through a breaking change, and we know which images need updating when there are known security vulnerabilities in old images. Now create a file and name it &quot;Dockerfile&quot; and put the following instructions inside it: Dockerfile # Start from the alpine image that is smaller but no fancy tools FROM alpine:3.19 # Use /usr/src/app as our workdir. The following instructions will be executed in this location. WORKDIR /usr/src/app # Copy the hello.sh file from this directory to /usr/src/app/ creating /usr/src/app/hello.sh COPY hello.sh . # Alternatively, if we skipped chmod earlier, we can add execution permissions during the build. # RUN chmod +x hello.sh # When running docker run the command will be ./hello.sh CMD ./hello.sh  Great! We can use the command docker build to turn the Dockerfile to an image. By default docker build will look for a file named Dockerfile. Now we can run docker build with instructions where to build (.) and give it a name (-t &lt;name&gt;): $ docker build . -t hello-docker =&gt; [internal] load build definition from Dockerfile 0.0s =&gt; =&gt; transferring dockerfile: 478B 0.0s =&gt; [internal] load metadata for docker.io/library/alpine:3.19 2.1s =&gt; [auth] library/alpine:pull token for registry-1.docker.io 0.0s =&gt; [internal] load .dockerignore 0.0s =&gt; =&gt; transferring context: 2B 0.0s =&gt; [1/3] FROM docker.io/library/alpine:3.19@sha256:c5b1261d6d3e43071626931fc004f70149baeba2c8ec672bd4f27761f8e1ad6b 0.0s =&gt; [internal] load build context 0.0s =&gt; =&gt; transferring context: 68B 0.0s =&gt; [2/3] WORKDIR /usr/src/app 0.0s =&gt; [3/3] COPY hello.sh . 0.0s =&gt; exporting to image 0.0s =&gt; =&gt; exporting layers 0.0s =&gt; =&gt; writing image sha256:5f8f5d7445f34b0bcfaaa4d685a068cdccc1ed79e65068337a3a228c79ea69c8 0.0s =&gt; =&gt; naming to docker.io/library/hello-docker  Let us ensure that the image exists: $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE hello-docker latest 5f8f5d7445f3 4 minutes ago 7.73MB  Permission denied If you're now getting &quot;/bin/sh: ./hello.sh: Permission denied&quot; it's because the chmod +x hello.sh was skipped earlier. You can simply uncomment the RUN instruction between COPY and CMD instructions not found If you're now getting &quot;/bin/sh: ./hello.sh: not found&quot; and you're using Windows it might be because by default Windows uses CRLF as line ending. Unix, in our case Alpine, uses just LF which makes the copying of our hello.sh invalid bash script in the build phase. To overcome this error change the line endings to LF before running docker build Now executing the application is as simple as running docker run hello-docker. Try it! During the build we see from the output that there are three steps: [1/3], [2/3] and [3/3]. The steps here represent layers of the image so that each step is a new layer on top of the base image (alpine:3.19 in our case). Layers have multiple functions. We often try to limit the number of layers to save on storage space but layers can work as a cache during build time. If we just edit the last lines of Dockerfile the build command can start from the previous layer and skip straight to the section that has changed. COPY automatically detects changes in the files, so if we change the hello.sh it'll run from step 3/3, skipping 1 and 2. This can be used to create faster build pipelines. We'll talk more about optimization in part 3. It is also possible to manually create new layers on top of a image. Let us now create a new file called additional.txt and copy it inside a container. We'll need two terminals, that shall be called 1 and 2 in the following listings. Let us start by running the image: # do this in terminal 1 $ docker run -it hello-docker sh /usr/src/app #  Now we're inside of the container. We replaced the CMD we defined earlier with sh and used -i and -t to start the container so that we can interact with it. In the second terminal we will copy the file inside the container: # do this in terminal 2 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9c06b95e3e85 hello-docker &quot;sh&quot; 4 minutes ago Up 4 minutes zen_rosalind $ touch additional.txt $ docker cp ./additional.txt zen_rosalind:/usr/src/app/  The file is created with command touch right before copying it in. Let us ensure that the file is copied inside the container: # do this in terminal 1 /usr/src/app # ls additional.txt hello.sh  Great! Now we've made a change to the container. We can use command docker diff to check what has changed # do this in terminal 2 $ docker diff zen_rosalind C /usr C /usr/src C /usr/src/app A /usr/src/app/additional.txt C /root A /root/.ash_history  The character in front of the file name indicates the type of the change in the container's filesystem: A = added, D = deleted, C = changed. The additional.txt was created and our ls created .ash_history. Next we will save the changes as a new image with the command docker commit: # do this in terminal 2 $ docker commit zen_rosalind hello-docker-additional sha256:2f63baa355ce5976bf89fe6000b92717f25dd91172aed716208e784315bfc4fd $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE hello-docker-additional latest 2f63baa355ce 3 seconds ago 7.73MB hello-docker latest 444f21cf7bd5 31 minutes ago 7.73MB  Technically the command docker commit added a new layer on top of the image hello-docker, and the resulting image was given the name hello-docker-additional. We will actually not use the command docker commit again during this course. This is because defining the changes to the Dockerfile is much more sustainable method of managing changes. No magic actions or scripts, just a Dockerfile that can be version controlled. Let's do just that and create hello-docker with v2 tag that includes the file additional.txt. The new file can be added with a RUN instruction: Dockerfile # Start from the alpine image FROM alpine:3.19 # Use /usr/src/app as our workdir. The following instructions will be executed in this location. WORKDIR /usr/src/app # Copy the hello.sh file from this location to /usr/src/app/ creating /usr/src/app/hello.sh. COPY hello.sh . # Execute a command with `/bin/sh -c` prefix. RUN touch additional.txt # When running Docker run the command will be ./hello.sh CMD ./hello.sh  Now we used the RUN instruction to execute the command touch additional.txt which creates a file inside the resulting image. Pretty much anything that can be executed in the container based on the created image, can be instructed to be run with the RUN instruction during the build of a Dockerfile. Build now the Dockerfile with docker build . -t hello-docker:v2 and we are done! Let's compare the output of ls: $ docker run hello-docker-additional ls additional.txt hello.sh $ docker run hello-docker:v2 ls additional.txt hello.sh  Now we know that all instructions in a Dockerfile except CMD (and one other that we will learn about soon) are executed during build time. CMD is executed when we call docker run, unless we overwrite it. "},{"title":"Exercises 1.7 - 1.8​","type":1,"pageTitle":"In-depth dive into images","url":"/part-1/section-3#exercises-17---18","content":"Exercise 1.7: Image for script We can improve our previous solutions now that we know how to create and build a Dockerfile. Let us now get back to Exercise 1.4. Create a new file script.sh on your local machine with the following contents: while true do echo &quot;Input website:&quot; read website; echo &quot;Searching..&quot; sleep 1; curl http://$website done Create a Dockerfile for a new image that starts from ubuntu:22.04 and add instructions to install curl into that image. Then add instructions to copy the script file into that image and finally set it to run on container start using CMD. After you have filled the Dockerfile, build the image with the name &quot;curler&quot;. If you are getting permission denied, use chmod to give permission to run the script. The following should now work: $ docker run -it curler Input website: helsinki.fi Searching.. &lt;!DOCTYPE HTML PUBLIC &quot;-//IETF//DTD HTML 2.0//EN&quot;&gt; &lt;html&gt;&lt;head&gt; &lt;title&gt;301 Moved Permanently&lt;/title&gt; &lt;/head&gt;&lt;body&gt; &lt;h1&gt;Moved Permanently&lt;/h1&gt; &lt;p&gt;The document has moved &lt;a href=&quot;https://www.helsinki.fi/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; Remember that RUN can be used to execute commands while building the image! Submit the Dockerfile. Exercise 1.8: Two line Dockerfile By default our devopsdockeruh/simple-web-service:alpine doesn't have a CMD. Instead, it uses ENTRYPOINT to declare which application is run. We'll talk more about ENTRYPOINT in the next section, but you already know that the last argument in docker run can be used to give a command or an argument. As you might've noticed it doesn't start the web service even though the name is &quot;simple-web-service&quot;. A suitable argument is needed to start the server! Try docker run devopsdockeruh/simple-web-service:alpine hello. The application reads the argument &quot;hello&quot; but will inform that hello isn't accepted. In this exercise create a Dockerfile and use FROM and CMD to create a brand new image that automatically runs server. The Docker documentation CMD says a bit indirectly that if a image has ENTRYPOINT defined, CMD is used to define it the default arguments. Tag the new image as &quot;web-server&quot; Return the Dockerfile and the command you used to run the container. Running the built &quot;web-server&quot; image should look like this: $ docker run web-server [GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached. [GIN-debug] [WARNING] Running in &quot;debug&quot; mode. Switch to &quot;release&quot; mode in production. - using env: export GIN_MODE=release - using code: gin.SetMode(gin.ReleaseMode) [GIN-debug] GET /*path --&gt; server.Start.func1 (3 handlers) [GIN-debug] Listening and serving HTTP on :8080 We don't have any method of accessing the web service yet. As such confirming that the console output is the same will suffice. The exercise title may be a useful hint here. "},{"title":"Summary","type":0,"sectionRef":"#","url":"/part-1/section-7","content":"","keywords":""},{"title":"ECTS Credits​","type":1,"pageTitle":"Summary","url":"/part-1/section-7#ects-credits","content":"Enrolling after each part is required for the ECTS credits. Now that you have completed part 1 use the following link to enroll in this course: DevOps with Docker, https://www.avoin.helsinki.fi/palvelut/esittely.aspx?s=otm-4bd45ab8-8b23-4973-a918-a6b6f7bbb347 If you wish to end in this part and not do the following parts, follow the instructions at the bottom of getting started page NOTE! Enrollment for the course through the Open University is possible until June 16th, 2024. Credits for the course are only available to those students who have successfully enrolled on the course through the Open University and have completed the course according to the instructions. "},{"title":"Defining start conditions for the container","type":0,"sectionRef":"#","url":"/part-1/section-4","content":"","keywords":""},{"title":"Improved curler​","type":1,"pageTitle":"Defining start conditions for the container","url":"/part-1/section-4#improved-curler","content":"With ENTRYPOINT we can make the curler of the Exercise 1.7. more flexible. Change the script so that it takes the first argument as the input: #!/bin/bash echo &quot;Searching..&quot;; sleep 1; curl http://$1;  And change the CMD to ENTRYPOINT with the format [&quot;./script.sh&quot;]. Now we can run $ docker build . -t curler-v2 $ docker run curler-v2 helsinki.fi Searching.. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 232 100 232 0 0 13647 0 --:--:-- --:--:-- --:--:-- 13647 &lt;!DOCTYPE HTML PUBLIC &quot;-//IETF//DTD HTML 2.0//EN&quot;&gt; &lt;html&gt;&lt;head&gt; &lt;title&gt;301 Moved Permanently&lt;/title&gt; &lt;/head&gt;&lt;body&gt; &lt;h1&gt;Moved Permanently&lt;/h1&gt; &lt;p&gt;The document has moved &lt;a href=&quot;https://www.helsinki.fi/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/body&gt;&lt;/html&gt;  "},{"title":"Introduction to Part 2","type":0,"sectionRef":"#","url":"/part-2/","content":"Introduction to Part 2 This part introduces container orchestration with Docker Compose and relevant concepts such as docker network. By the end of this part you are able to: Run a group of containerized applications that interact with each other via HTTP Run a group of containerized applications that interact with each other via volumes Manually scale applications Use 3rd party services, such as databases, inside containers as part of your project","keywords":""},{"title":"Utilizing tools from the Registry","type":0,"sectionRef":"#","url":"/part-1/section-6","content":"","keywords":""},{"title":"Exercises 1.11-1.14​","type":1,"pageTitle":"Utilizing tools from the Registry","url":"/part-1/section-6#exercises-111-114","content":"Exercise 1.11: Spring Create a Dockerfile for an old Java Spring project that can be found from the course repository. The setup should be straightforward with the README instructions. Tips to get you started: There are many options for running Java, you may use eg. amazoncorretto FROM amazoncorretto:_tag_ to get Java instead of installing it manually. Pick the tag by using the README and Docker Hub page. You've completed the exercise when you see a 'Success' message in your browser. Submit the Dockerfile you used to run the container. The following three exercises will start a larger project that we will configure in parts 2 and 3. They will require you to use everything you've learned up until now. If you need to modify a Dockerfile in some later exercises, feel free to do it on top of the Dockerfiles you create here. Mandatory exercises The next exercises are the first mandatory ones. Mandatory exercises can not be skipped. Mandatory Exercise 1.12: Hello, frontend! A good developer creates well-written READMEs. Such that they can be used to create Dockerfiles with ease. Clone, fork or download the project fromhttps://github.com/docker-hy/material-applications/tree/main/example-frontend. Create a Dockerfile for the project (example-frontend) and give a command so that the project runs in a Docker container with port 5000 exposed and published so when you start the container and navigate to http://localhost:5000you will see message if you're successful. note that the port 5000 is reserved in the more recent OSX versions (Monterey, Big Sur), so you have to use some other host port Submit the Dockerfile. As in other exercises, do not alter the code of the project TIPS: The project has install instructions in README.Note that the app starts to accept connections when &quot;Accepting connections at http://localhost:5000&quot; has been printed to the screen, this takes a few secondsYou do not have to install anything new outside containers.The project might not work with too new Node.js versions Mandatory Exercise 1.13: Hello, backend! Clone, fork or download a project fromhttps://github.com/docker-hy/material-applications/tree/main/example-backend. Create a Dockerfile for the project (example-backend). Start the container with port 8080 published. When you start the container and navigate to http://localhost:8080/ping you should get a &quot;pong&quot; as a response. Submit the Dockerfile and the command used. Do not alter the code of the project TIPS: you might need thisIf you have M1/M2 Mac, you might need to build the image with an extra option docker build --platform linux/amd64 -t imagename . Mandatory Exercise 1.14: Environment Start both the frontend and the backend with the correct ports exposed and add ENV to Dockerfile with the necessary information from both READMEs (front, back). Ignore the backend configurations until the frontend sends requests to _backend_url_/ping when you press the button. You know that the configuration is ready when the button for 1.14 of frontend responds and turns green. Do not alter the code of either project Submit the edited Dockerfiles and commands used to run. The frontend will first talk to your browser. Then the code will be executed from your browser and that will send a message to the backend. TIPS: When configuring web applications keep the browser developer console ALWAYS open, F12 or cmd+shift+I when the browser window is open. Information about configuring cross-origin requests is in the README of the backend project.The developer console has multiple views, the most important ones are Console and Network. Exploring the Network tab can give you a lot of information on where messages are being sent and what is received as a response! "},{"title":"Publishing projects​","type":1,"pageTitle":"Utilizing tools from the Registry","url":"/part-1/section-6#publishing-projects","content":"Go to https://hub.docker.com/ to create an account. You can configure Docker hub to build your images for you, but using push works as well. Let's publish the youtube-dl image. Log in and navigate to your dashboard and press Create Repository. The namespace can be either your personal account or an organization account. For now, let's stick to personal accounts and write something descriptive such as youtube-dl to repository name. We will need to remember it in part 2. Set visibility to public. And the last thing we need is to authenticate our push by logging in: $ docker login  Next, you will need to rename the image to include your username, and then you can push it: $ docker tag youtube-dl &lt;username&gt;/&lt;repository&gt; ... $ docker push &lt;username&gt;/&lt;repository&gt; ...  "},{"title":"Exercises 1.15-1.16​","type":1,"pageTitle":"Utilizing tools from the Registry","url":"/part-1/section-6#exercises-115-116","content":"Exercise 1.15: Homework Create Dockerfile for an application or any other dockerised project in any of your own repositories and publish it to Docker Hub. This can be any project, except the clones or forks of backend-example or frontend-example. For this exercise to be complete you have to provide the link to the project in Docker Hub, make sure you at least have a basic description and instructions for how to run the application in a README that's available through your submission. Exercise 1.16: Cloud deployment It is time to wrap up this part and run a containerized app in the cloud. You can take any web-app, eg. an example or exercise from this part, your own app, or even the course material (see devopsdockeruh/coursepage) and deploy it to some cloud provider. There are plenty of alternatives, and most provide a free tier. Here are some alternatives that are quite simple to use: fly.io (easy to use but needs a credit card even in the free tier)render.com (bad documentation, you most likely need google)heroku.com (has a free student plan through GitHub Student Developer Pack) If you know a good cloud service for the purposes of this exercise, please tell us (yes, we know about Amazon AWS, Google Cloud and Azure already... ). Submit the Dockerfile, a brief description of what you did, and a link to the running app. "},{"title":"Containers in development","type":0,"sectionRef":"#","url":"/part-2/section-4","content":"","keywords":""},{"title":"Exercise 2.11​","type":1,"pageTitle":"Containers in development","url":"/part-2/section-4#exercise-211","content":"Exercise 2.11 Select some of your own development projects and start utilizing containers in the development environment. Explain what you have done. It can be anything, e.g., support for docker-compose.yml to have services (such as databases) containerized or even a fully blown containerized development environment. If you are interested in how to build a containerized development environment for a React/Node Single page web app, please have a look at the course Full stack open which has one chapter devoted to the topic. "},{"title":"Migrating to Docker Compose","type":0,"sectionRef":"#","url":"/part-2/section-1","content":"","keywords":""},{"title":"Volumes in Docker Compose​","type":1,"pageTitle":"Migrating to Docker Compose","url":"/part-2/section-1#volumes-in-docker-compose","content":"To run the image as we did previously, we will need to add the volume bind mounts. Volumes in Docker Compose are defined with the following syntax location-in-host:location-in-container. Compose can work without an absolute path: version: '3.8' services: yt-dlp-ubuntu: image: &lt;username&gt;/&lt;repositoryname&gt; build: . volumes: - .:/mydir container_name: yt-dlp  We can also give the container a name it will use when running with container_name. The service name can be used to run it: $ docker compose run yt-dlp-ubuntu https://imgur.com/JY5tHqr  "},{"title":"Exercise 2.1​","type":1,"pageTitle":"Migrating to Docker Compose","url":"/part-2/section-1#exercise-21","content":"Exercise 2.1 Let us now leverage the Docker Compose with the simple webservice that we used in the Exercise 1.3 Without a command devopsdockeruh/simple-web-service will create logs into its /usr/src/app/text.log. Create a docker-compose.yml file that starts devopsdockeruh/simple-web-service and saves the logs into your filesystem. Submit the docker-compose.yml, and make sure that it works simply by running docker compose up if the log file exists. "},{"title":"Web services in Docker Compose​","type":1,"pageTitle":"Migrating to Docker Compose","url":"/part-2/section-1#web-services-in-docker-compose","content":"Compose is really meant for running web services, so let's move from simple binary wrappers to running a HTTP service. https://github.com/jwilder/whoami is a simple service that prints the current container id (hostname). $ docker container run -d -p 8000:8000 jwilder/whoami 736ab83847bb12dddd8b09969433f3a02d64d5b0be48f7a5c59a594e3a6a3541  Navigate with a browser or curl to localhost:8000, they both will answer with the id. Take down the container so that it's not blocking port 8000. $ docker container stop 736ab83847bb $ docker container rm 736ab83847bb  Let's create a new folder and a Docker Compose file whoami/docker-compose.yml from the command line options. version: '3.8' services: whoami: image: jwilder/whoami ports: - 8000:8000  Test it: $ docker compose up -d $ curl localhost:8000  Environment variables can also be given to the containers in Docker Compose as follows: version: '3.8' services: backend: image: environment: - VARIABLE=VALUE - VARIABLE2=VALUE2  Note that there are also other, perhaps more elegant ways to define the environment variables in Docker compose. "},{"title":"Exercises 2.2 - 2.3​","type":1,"pageTitle":"Migrating to Docker Compose","url":"/part-2/section-1#exercises-22---23","content":"Exercise 2.2 Read about how to add the command to docker-compose.yml from the documentation. The familiar image devopsdockeruh/simple-web-service can be used to start a web service, see the exercise 1.10. Create a docker-compose.yml, and use it to start the service so that you can use it with your browser. Submit the docker-compose.yml, and make sure that it works simply by running docker compose up Mandatory Exercise 2.3 As we saw previously, starting an application with two programs was not trivial and the commands got a bit long. In the previous part we created Dockerfiles for both frontend and backend of the example application. Next, simplify the usage into one docker-compose.yml. Configure the backend and frontend from part 1 to work in Docker Compose. Submit the docker-compose.yml "},{"title":"Docker networking","type":0,"sectionRef":"#","url":"/part-2/section-2","content":"","keywords":""},{"title":"Exercise 2.4​","type":1,"pageTitle":"Docker networking","url":"/part-2/section-2#exercise-24","content":"Exercise 2.4 In this exercise you should expand the configuration done in Exercise 2.3 and set up the example backend to use the key-value database Redis. Redis is quite often used as a cache to store data so that future requests for data can be served faster. The backend uses a slow API to fetch some information. You can test the slow API by requesting /ping?redis=true with curl. The frontend app has a button to test this. So you should improve the performance of the app and configure a Redis container to cache information for the backend. Thedocumentation of the Redis image might contain some useful info. The backend README should have all the information that is needed for configuring the backend. When you've correctly configured the button will turn green. Submit the docker-compose.yml The restart: unless-stopped configuration can help if the Redis takes a while to get ready. "},{"title":"Manual network definition​","type":1,"pageTitle":"Docker networking","url":"/part-2/section-2#manual-network-definition","content":"It is also possible to define the network manually in a Docker Compose file. A major benefit of a manual network definition is that it makes it easy to set up a configuration where containers defined in two different Docker Compose files share a network, and can easily interact with each other. Let us now have a look how a network is defined in docker-compose.yml: version: &quot;3.8&quot; services: db: image: postgres:13.2-alpine networks: - database-network # Name in this Docker Compose file networks: database-network: # Name in this Docker Compose file name: database-network # Name that will be the actual name of the network  This defines a network called database-network which is created with docker compose up and removed with docker compose down. As can be seen, services are configured to use a network by adding networks into the definition of the service. Establishing a connection to an external network (that is, a network defined in another docker-compose.yml, or by some other means) is done as follows: version: &quot;3.8&quot; services: db: image: backend-image networks: - database-network networks: database-network: external: name: database-network # Must match the actual name of the network  By default all services are added to a network called default. The default network can be configured and this makes it possible to connect to an external network by default as well: version: &quot;3.8&quot; services: db: image: backend-image networks: default: external: name: database-network # Must match the actual name of the network  "},{"title":"Scaling​","type":1,"pageTitle":"Docker networking","url":"/part-2/section-2#scaling","content":"Compose can also scale the service to run multiple instances: $ docker compose up --scale whoami=3 WARNING: The &quot;whoami&quot; service specifies a port on the host. If multiple containers for this service are created on a single host, the port will clash. Starting whoami_whoami_1 ... done Creating whoami_whoami_2 ... error Creating whoami_whoami_3 ... error  The command fails due to a port clash, as each instance will attempt to bind to the same host port (8000). We can get around this by only specifying the container port. As mentioned in part 1, when leaving the host port unspecified, Docker will automatically choose a free port. Update the ports definition in docker-compose.yml: ports: - 8000  Then run the command again: $ docker compose up --scale whoami=3 Starting whoami_whoami_1 ... done Creating whoami_whoami_2 ... done Creating whoami_whoami_3 ... done  All three instances are now running and listening on random host ports. We can use docker compose port to find out which ports the instances are bound to. $ docker compose port --index 1 whoami 8000 0.0.0.0:32770 $ docker compose port --index 2 whoami 8000 0.0.0.0:32769 $ docker compose port --index 3 whoami 8000 0.0.0.0:32768  We can now curl from these ports: $ curl 0.0.0.0:32769 I'm 536e11304357 $ curl 0.0.0.0:32768 I'm 1ae20cd990f7  In a server environment you'd often have a load balancer in front of the service. For containerized local environment (or a single server) one good solution is to use https://github.com/jwilder/nginx-proxy. Let's add the nginx-proxy to our compose file and remove the port bindings from the whoami service. We'll mount our docker.sock (the socket that is used to communicate with the Docker Daemon) inside of the container in :ro read-only mode: version: &quot;3.8&quot; services: whoami: image: jwilder/whoami proxy: image: jwilder/nginx-proxy volumes: - /var/run/docker.sock:/tmp/docker.sock:ro ports: - 80:80  Let test the configuration: $ docker compose up -d --scale whoami=3 $ curl localhost:80 &lt;html&gt; &lt;head&gt;&lt;title&gt;503 Service Temporarily Unavailable&lt;/title&gt;&lt;/head&gt; &lt;body bgcolor=&quot;white&quot;&gt; &lt;center&gt;&lt;h1&gt;503 Service Temporarily Unavailable&lt;/h1&gt;&lt;/center&gt; &lt;hr&gt;&lt;center&gt;nginx/1.13.8&lt;/center&gt; &lt;/body&gt; &lt;/html&gt;  It's &quot;working&quot;, but the Nginx just doesn't know which service we want. The nginx-proxy works with two environment variables: VIRTUAL_HOST and VIRTUAL_PORT. VIRTUAL_PORT is not needed if the service has EXPOSE in it's Docker image. We can see that jwilder/whoami sets it: https://github.com/jwilder/whoami/blob/master/Dockerfile#L9 Note: Mac users with the M1 processor you may see the following error message: runtime: failed to create new OS thread. In this case you can use the Docker Image ninanung/nginx-proxy instead which offers a temporary fix until jwilder/nginx-proxy is updated to support M1 Macs. The domain colasloth.com is configured so that all subdomains point to 127.0.0.1. More information about how this works can be found at colasloth.github.io, but in brief it's a simple DNS &quot;hack&quot;. Several other domains serving the same purpose exist, such as localtest.me, lvh.me, and vcap.me, to name a few. In any case, let's use colasloth.com here: version: &quot;3.8&quot; services: whoami: image: jwilder/whoami environment: - VIRTUAL_HOST=whoami.colasloth.com proxy: image: jwilder/nginx-proxy volumes: - /var/run/docker.sock:/tmp/docker.sock:ro ports: - 80:80  Now the proxy works: $ docker compose up -d --scale whoami=3 $ curl whoami.colasloth.com I'm f6f85f4848a8 $ curl whoami.colasloth.com I'm 740dc0de1954  Let's add couple of more containers behind the same proxy. We can use the official nginx image to serve a simple static web page. We don't have to even build the container images, we can just mount the content to the image. Let's prepare some content for two services called &quot;hello&quot; and &quot;world&quot;. $ echo &quot;hello&quot; &gt; hello.html $ echo &quot;world&quot; &gt; world.html  Then add these services to the docker-compose.yml file where you mount just the content as index.html in the default nginx path: hello: image: nginx:1.19-alpine volumes: - ./hello.html:/usr/share/nginx/html/index.html:ro environment: - VIRTUAL_HOST=hello.colasloth.com world: image: nginx:1.19-alpine volumes: - ./world.html:/usr/share/nginx/html/index.html:ro environment: - VIRTUAL_HOST=world.colasloth.com  Now let's test: $ docker compose up -d --scale whoami=3 $ curl hello.colasloth.com hello $ curl world.colasloth.com world $ curl whoami.colasloth.com I'm f6f85f4848a8 $ curl whoami.colasloth.com I'm 740dc0de1954  Now we have a basic single-machine hosting setup up and running. Test updating the hello.html without restarting the container, does it work? "},{"title":"Exercises 2.5​","type":1,"pageTitle":"Docker networking","url":"/part-2/section-2#exercises-25","content":"Exercise 2.5 The project https://github.com/docker-hy/material-applications/tree/main/scaling-exercise is a barely working application. Go ahead and clone it for yourself. The project already includes docker-compose.yml so you can start it by running docker compose up. The application should be accessible through http://localhost:3000. However it doesn't work well enough and we've added a load balancer for scaling. Your task is to scale the compute containers so that the button in the application turns green. This exercise was created with Sasu Mäkinen Please return the used commands for this exercise. "},{"title":"Summary","type":0,"sectionRef":"#","url":"/part-2/section-5","content":"","keywords":""},{"title":"ECTS Credits​","type":1,"pageTitle":"Summary","url":"/part-2/section-5#ects-credits","content":"Enrolling after each part is required for the ECTS credits. Now that you have completed part 2 use the following link to enroll in this course: DevOps with Docker: Docker Compose, https://www.avoin.helsinki.fi/palvelut/esittely.aspx?s=otm-c73ef1c6-8fb0-42e8-9052-ef59b01cb409 If you wish to end in this part and not do the following parts, follow the instructions at the bottom of getting started page NOTE! Enrollment for the course through the Open University is possible until June 16th, 2024. Credits for the course are only available to those students who have successfully enrolled in the course through the Open University and have completed the course according to the instructions. "},{"title":"Introduction to Part 3","type":0,"sectionRef":"#","url":"/part-3/","content":"Introduction to Part 3 This part introduces production-ready practices such as container optimization and deployment pipelines. We'll also familiarize ourselves with other container orchestration solutions. By the end of this part you are able to: Critically examine the images that you pull Trim the container size and image build time via multiple methods such as multi-stage builds. Automatically deploy containers","keywords":""},{"title":"Volumes in action","type":0,"sectionRef":"#","url":"/part-2/section-3","content":"","keywords":""},{"title":"Exercises 2.6 - 2.10​","type":1,"pageTitle":"Volumes in action","url":"/part-2/section-3#exercises-26---210","content":"Exercise 2.6 Let us continue with the example app that we worked with in Exercise 2.4. Now you should add a database to the example backend. Use a Postgres database to save messages. For now, there is no need to configure a volume since the official Postgres image sets a default volume for us. Use the Postgres image documentation to your advantage when configuring: https://hub.docker.com/_/postgres/. Especially part Environment Variables is a valuable one. The backend README should have all the information needed to connect. There is again a button (and a form!) in the frontend that you can use to ensure your configuration is done right. Submit the docker-compose.yml TIPS: When configuring the database, you might need to destroy the automatically created volumes. Use commands docker volume prune, docker volume ls and docker volume rm to remove unused volumes when testing. Make sure to remove containers that depend on them beforehand.restart: unless-stopped can help if the Postgres takes a while to get ready Exercise 2.7 Postgres image uses a volume by default. Define manually a volume for the database in a convenient location such as in ./database so you should use now a bind mount. The image documentation may help you with the task. After you have configured the bind mount volume: Save a few messages through the frontendRun docker compose downRun docker compose up and see that the messages are available after refreshing browserRun docker compose down and delete the volume folder manuallyRun docker compose up and the data should be gone TIP: To save you the trouble of testing all of those steps, just look into the folder before trying the steps. If it's empty after docker compose up then something is wrong. Submit the docker-compose.yml The benefit of a bind mount is that since you know exactly where the data is in your file system, it is easy to create backups. If the Docker managed volumes are used, the location of the data in the file system can not be controlled and that makes backups a bit less trivial... Tips for making sure the backend connection works In the next exercise try using your browser to access http://localhost/api/ping and see if it answers pong It might be Nginx configuration problem. Ensure there is a trailing / on the backend URL as specified under the location /api/ context in the nginx.conf. Exercise 2.8 In this exercise, you shall add Nginx to work as a reverse proxy in front of the example app frontend and backend. According to Wikipedia a reverse proxy is a type of proxy server that retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the reverse proxy server itself. So in our case, the reverse proxy will be the single point of entry to our application, and the final goal will be to set both the React frontend and the Express backend behind the reverse proxy. The idea is that a browser makes all requests to http://localhost. If the request has a URL prefix http://localhost/api, Nginx should forward the request to the backend container. All the other requests are directed to the frontend container. So, at the end, you should see that the frontend is accessible simply by going to http://localhost. All buttons, except the one labeled Exercise 2.8 may have stopped working, do not worry about them, we shall fix that later. The following file should be set to /etc/nginx/nginx.conf inside the Nginx container. You can use a file bind mount where the contents of the file is the following: events { worker_connections 1024; } http { server { listen 80; location / { proxy_pass _frontend-connection-url_; } # configure here where requests to http://localhost/api/... # are forwarded location /api/ { proxy_set_header Host $host; proxy_pass _backend-connection-url_; } } } Nginx, backend and frontend should be connected in the same network. See the image above for how the services are connected. You find Nginx-documentation helpful, but remember, the configuration you need is pretty straightforward, if you end up doing complex things, you are most likely doing something wrong. If and when your app &quot;does not work&quot;, remember to have a look in the log, it can be pretty helpful in pinpointing errors: 2_7-proxy-1 | /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh 2_7-proxy-1 | /docker-entrypoint.sh: Configuration complete; ready for start up 2_7-proxy-1 | 2023/03/05 09:24:51 [emerg] 1#1: invalid URL prefix in /etc/nginx/nginx.conf:8 2_7-proxy-1 exited with code 1 Submit the docker-compose.yml Exercise 2.9 Most of the buttons may have stopped working in the example application. Make sure that every button for exercises works. Remember to take a peek into the browser's developer consoles again like we did back part 1, remember also this and this. The buttons of the Nginx exercise and the first button behave differently but you want them to match. If you had to make any changes explain what you did and where. Submit the docker-compose.yml and both Dockerfiles. Publishing ports to host network There is an important lesson about Docker networking and ports to be learned in the next exercise. When we do a port mapping, in docker run -p 8001:80 ... or in the Docker Compose file, we publish a container port to the host network to be accessible in localhost. The container port is there within the Docker network accessible by the other containers that are in the same network even if we do not publish anything. So publishing the ports is only for exposing ports outside the Docker network. If no direct access outside the network is not needed, then we just do not publish anything. Exercise 2.10 Now we have the reverse proxy up and running! All the communication to our app should be done through the reverse proxy and direct access (eg. accessing the backend with a GET to http://localhost:8080/ping ) should be prevented. Use a port scanner, eg https://hub.docker.com/r/networkstatic/nmap to ensure that there are no extra ports open in the host. It might be enough to just run $ docker run -it --rm --network host networkstatic/nmap localhost If you have an M1/M2 Mac, you might need to build the image yourself. The result looks like the following (I used a self-built image): $ docker run -it --rm --network host nmap localhost Starting Nmap 7.93 ( https://nmap.org ) at 2023-03-05 12:28 UTC Nmap scan report for localhost (127.0.0.1) Host is up (0.0000040s latency). Other addresses for localhost (not scanned): ::1 Not shown: 996 closed tcp ports (reset) PORT STATE SERVICE 80/tcp filtered http 111/tcp open rpcbind 5000/tcp filtered commplex-link 8080/tcp filtered http-proxy Nmap done: 1 IP address (1 host up) scanned in 1.28 seconds As we see, there are two suspicious open ports: 5000 and 8080. So it is obvious that the frontend and backend are still directly accessible in the host network. This should be fixed! You are done when the port scan report looks something like this: Starting Nmap 7.93 ( https://nmap.org ) at 2023-03-05 12:39 UTC Nmap scan report for localhost (127.0.0.1) Host is up (0.0000040s latency). Other addresses for localhost (not scanned): ::1 Not shown: 998 closed tcp ports (reset) PORT STATE SERVICE 80/tcp filtered http 111/tcp open rpcbind Nmap done: 1 IP address (1 host up) scanned in 1.28 seconds  "},{"title":"Official Images and trust","type":0,"sectionRef":"#","url":"/part-3/section-1","content":"","keywords":""},{"title":"Look into official images​","type":1,"pageTitle":"Official Images and trust","url":"/part-3/section-1#look-into-official-images","content":"Which version is considered to be the &quot;official&quot; version is up to the maintainers of Docker Official Images to decide. The official images repository contains a library of images considered official. They are introduced into the library by regular pull request processes. The extended process for verifying an image is described in the repository README. Many of the most well-known projects are maintained under the docker-library organization. Those include images such as Postgres and Python. However, many of them are included in the library but are managed by a separate organization, like Ubuntu and Node.js Let's look into the Ubuntu image on Docker Hub and trace the process. The description/readme says: What's in this image? This image is built from official rootfs tarballs provided by Canonical (see dist-* tags at https://git.launchpad.net/cloud-images/+oci/ubuntu-base).  We can see that the image is built from https://git.launchpad.net/cloud-images/+oci/ubuntu-base. Let's take a closer look at Ubuntu to verify where it comes from. If you click the Dockerfile link of https://hub.docker.com/r/_/ubuntu/ you'll be given a json file instead of the Dockerfile contents. The digest seems to contain the valid digest for a single architecture version listed on Docker Hub (amd64). But after downloading it and checking the digest docker pull ubuntu:22.04 &amp;&amp; docker image ls --digests the result does not match up. We find a Dockerfile from the repository here. We can increase trust that it's the same as the image we downloaded with image history: $ docker image history --no-trunc ubuntu:22.04  The output from image history matches with the directives specified in the Dockerfile. If this isn't enough, we could also build the image ourselves. The first line states that the image starts FROM a special image &quot;scratch&quot; that is just empty. Then a file ubuntu-*-oci-$LAUNCHPAD_BUILD_ARCH-root.tar.gz is added to the root from the same directory. Notice how the file is not extracted at any point. The ADD instruction documentation states that &quot;If src is a local tar archive in a recognized compression format (identity, gzip, bzip2 or xz) then it is unpacked as a directory.&quot; We could verify the checksums of the file if we were interested. For the Ubuntu image automation from the launchpad takes care of creating the PRs to docker-library and the maintainers of the official images repository verify the PRs. You can also visit the Docker Hub page for the image tag itself, which shows the layers and warns about potential security issues. You can see how many different problems it finds here. Now we have learned that the build processes are open and we can verify it if we have the need. In addition, we learned that there's nothing that makes the &quot;official&quot; images special. &quot;You can't trust code that you did not totally create yourself.&quot;​ - Ken Thompson (1984, Reflections on Trusting Trust)​ "},{"title":"Using a non-root user","type":0,"sectionRef":"#","url":"/part-3/section-3","content":"","keywords":""},{"title":"Exercise 3.5​","type":1,"pageTitle":"Using a non-root user","url":"/part-3/section-3#exercise-35","content":"Mandatory Exercise 3.5 In exercises 1.12 and 1.13 we created Dockerfiles for both frontend and backend. Security issues with the user being a root are serious for the example frontend and backend as the containers for web services are supposed to be accessible through the Internet. Make sure the containers start their processes as non-root user. The backend image is based on Alpine Linux, which does not support the command useradd. Google will surely help you a way to create a user in an alpine based image. Submit the Dockerfiles. "},{"title":"End","type":0,"sectionRef":"#","url":"/part-3/section-6","content":"","keywords":""},{"title":"ECTS Credits​","type":1,"pageTitle":"End","url":"/part-3/section-6#ects-credits","content":"Enrolling after each part is required for the ECTS credits. Now that you have completed part 3 use the following link to enroll in this course: DevOps with Docker: security and optimization, https://www.avoin.helsinki.fi/palvelut/esittely.aspx?s=otm-487d8dd8-3a4b-447a-9118-f7bfff8169b6 NOTE! Enrollment for the course through the Open University is possible until June 16th, 2024. Credits for the course are only available to those students who have successfully enrolled in the course through the Open University and have completed the course according to the instructions. "},{"title":"Multi-host environments","type":0,"sectionRef":"#","url":"/part-3/section-5","content":"","keywords":""},{"title":"Exercise 3.11​","type":1,"pageTitle":"Multi-host environments","url":"/part-3/section-5#exercise-311","content":"Exercise 3.11: Kubernetes Familiarize yourself with Kubernetes terminology and draw a diagram describing what &quot;parts&quot; the Kubernetes contain and how those are related to each other. You should draw a diagram of at least three host machines in a Kubernetes cluster. In the diagram assume that the cluster is running two applications. The applications can be anything you want. An example could be a video game server and a blog website. You may take inspiration from the diagrams of part 2. The applications may utilize other machines or APIs that are not part of the cluster. At least three of the machines should be utilized. Include &quot;your own computer&quot; in the diagram as the one sending instructions via kubectl to deploy an application. In addition, include a HTTP message coming from the internet to your Kubernetes cluster and how it may reach an application. Make sure to label the diagram so that anyone else who has completed this exercise, and read the glossary, would understand it. The diagram should contain at least four of the following labels: Pod, Cluster, Container, Service and Volume. Glossary. And some helpful diagrams I prefer to use draw.io but you can use whichever tool you want. If you are interested, later this year the course DevOps With Kubernetes will provide a comprehensive treatment on using Kubernetes. "},{"title":"Deployment pipelines","type":0,"sectionRef":"#","url":"/part-3/section-2","content":"","keywords":""},{"title":"Exercises 3.1-3.4​","type":1,"pageTitle":"Deployment pipelines","url":"/part-3/section-2#exercises-31-34","content":"Exercise 3.1: Your pipeline Create now a similar deployment pipeline for a simple Node.js/Express app foundhere. Either clone the project or copy the files to your own repository. Set up a similar deployment pipeline (or the &quot;first half&quot;) using GitHub Actions that was just described. Ensure that a new image gets pushed to Docker Hub every time you push the code to GitHub (you may eg. change the message the app shows). Note that there is an important change that you should make to the above workflow configuration, the branch should be named main: name: Release Node.js app on: push: branches: - main jobs: build: runs-on: ubuntu-latest steps: # ... The earlier example still uses the old GitHub naming convention and calls the main branch master. Some of the actions that the above example uses are a bit outdated, so go through the documentation actions/checkoutdocker/login-actiondocker/build-push-action and use the most recent versions in your workflow. Keep an eye on the GitHub Actions page to see that your workflow is working: Ensure also from Docker Hub that your image gets pushed there. Next, run your image locally in detached mode, and ensure that you can access it with the browser. Now set up and run the Watchtower just as described above. You might do these two in a single step in a shared Docker Compose file. Now your deployment pipeline is set up! Ensure that it works: make a change to your code commit and push the changes to GitHub wait for some time (the time it takes for GitHub Action to build and push the image plus the Watchtower poll interval) reload the browser to ensure that Watchtower has started the new version (that is, your changes are visible) Submit a link to the repository with the config. Exercise 3.2: A deployment pipeline to a cloud service In Exercise 1.16 you deployed a containerized app to a cloud service. Now it is time to improve your solution by setting up a deployment pipeline for it so that every push to GitHub results in a new deployment to the cloud service. You will most likely find a ready-made GitHub Action that does most of the heavy lifting your you... Google is your friend! Submit a link to the repository with the config. The repository README should have a link to the deployed application. Exercise 3.3: Scripting magic Create a now script/program that downloads a repository from GitHub, builds a Dockerfile located in the root and then publishes it into the Docker Hub. You can use any scripting or programming language to implement the script. Using shell script might make the next exercise a bit easier... and do not worry if you have not done a shell script earlier, you do not need much for this exercise and Google helps. The script could eg. be designed to be used so that as the first argument it gets the GitHub repository and as the second argument the Docker Hub repository. Eg. when run as follows ./builder.sh mluukkai/express_app mluukkai/testing the script clones https://github.com/mluukkai/express_app, builds the image, and pushes it to Docker Hub repository mluukkai/testing Exercise 3.4: Building images from inside of a container As seen from the Docker Compose file, the Watchtower uses a volume to docker.sock socket to access the Docker daemon of the host from the container: services: watchtower: image: containrrr/watchtower volumes: - /var/run/docker.sock:/var/run/docker.sock # ... In practice this means that Watchtower can run commands on Docker the same way we can &quot;command&quot; Docker from the cli with docker ps, docker run etc. We can easily use the same trick in our own scripts! So if we mount the docker.sock socket to a container, we can use the command docker inside the container, just like we are using it in the host terminal! Dockerize now the script you did for the previous exercise. You can use images from this repository to run Docker inside Docker! Your Dockerized could be run like this (the command is divided into many lines for better readability, note that copy-pasting a multiline command does not work): docker run -e DOCKER_USER=mluukkai \\ -e DOCKER_PWD=password_here \\ -v /var/run/docker.sock:/var/run/docker.sock \\ builder mluukkai/express_app mluukkai/testing Note that now the Docker Hub credentials are defined as environment variables since the script needs to log in to Docker Hub for the push. Submit the Dockerfile and the final version of your script. Hint: you quite likely need to use ENTRYPOINT in this Exercise. See Part 1 for more. "},{"title":"Optimizing the image size","type":0,"sectionRef":"#","url":"/part-3/section-4","content":"","keywords":""},{"title":"Exercise 3.6​","type":1,"pageTitle":"Optimizing the image size","url":"/part-3/section-4#exercise-36","content":"Exercise 3.6 Return now back to our frontend andbackend Dockerfile. Document both image sizes at this point, as was done in the material. Optimize the Dockerfiles of both app frontend and backend, by joining the RUN commands and removing useless parts. After your improvements document the image sizes again. "},{"title":"Alpine Linux variant​","type":1,"pageTitle":"Optimizing the image size","url":"/part-3/section-4#alpine-linux-variant","content":"Our Ubuntu base image adds the most megabytes to our image. Alpine Linux provides a popular alternative base in https://hub.docker.com/_/alpine/ that is around 8 megabytes. It's based on alternative glibc implementation musl and busybox binaries, so not all software runs well (or at all) with it, but our container should run just fine. We'll create the following Dockerfile.alpine file: FROM alpine:3.19 WORKDIR /mydir RUN apk add --no-cache curl python3 ca-certificates &amp;&amp; \\ curl -L https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp -o /usr/local/bin/yt-dlp &amp;&amp; \\ chmod a+x /usr/local/bin/yt-dlp &amp;&amp; \\ adduser -D appuser &amp;&amp; \\ chown appuser . &amp;&amp; \\ apk del curl USER appuser ENTRYPOINT [&quot;/usr/local/bin/yt-dlp&quot;]  Size of the resulting image is 57.6MB Notes: The package manager is apk and it can work without downloading sources (caches) first with --no-cache.For creating user the command useradd is missing, but adduser can be used instead.Most of the package names are the same - there's a good package browser at https://pkgs.alpinelinux.org/packages. We build this file with :alpine-3.19 as the tag: $ docker build -t yt-dlp:alpine-3.19 -f Dockerfile.alpine .  It seems to run fine: $ docker run -v &quot;$(pwd):/mydir&quot; yt-dlp:alpine-3.19 https://www.youtube.com/watch\\?v\\=bNw2i-mRT4I  From the history, we can see that our single RUN layer size is 49.8MB $ docker image history yt-dlp:alpine-3.19 ... &lt;missing&gt; 6 minutes ago RUN /bin/sh -c apk add --no-cache curl pytho… 49.8MB buildkit.dockerfile.v0 ... &lt;missing&gt; 7 weeks ago /bin/sh -c #(nop) ADD file:d0764a717d1e9d0af… 7.73MB  So in total, our Alpine variant is about 57.6 megabytes, significantly less than our Ubuntu-based image. "},{"title":"Image with preinstalled environment​","type":1,"pageTitle":"Optimizing the image size","url":"/part-3/section-4#image-with-preinstalled-environment","content":"As seen, yt-dlp requires Python to function. Installing Python to Ubuntu- or Alpine-based image is very easy, it can be done with a single command. In general, installing the environment that is required to build and run a program inside a container can be quite a burden. Luckily, there are preinstalled images for many programming languages readily available on DockerHub, and instead of relying upon &quot;manual&quot; installation steps in a Dockerfile, it's quite often a good idea to use a pre-installed image. Let us use the one made for Python to run the yt-dpl: # we are using a new base image FROM python:3.12-alpine WORKDIR /mydir # no need to install python3 anymore RUN apk add --no-cache curl ca-certificates &amp;&amp; \\ curl -L https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp -o /usr/local/bin/yt-dlp &amp;&amp; \\ chmod a+x /usr/local/bin/yt-dlp &amp;&amp; \\ adduser -D appuser &amp;&amp; \\ chown appuser . &amp;&amp; \\ apk del curl USER appuser ENTRYPOINT [&quot;/usr/local/bin/yt-dlp&quot;]  There are many variants for the Python images, we have selected python:3.12-alpine which has Python version 3.12 and is based on Alpine Linux. The resulting image size is 59.5MB so it is slightly larger than the previous one where we installed Python by ourselves. Back in part 1, we published the Ubuntu version of yl-dlp with the tag latest. We can publish whatever variants we want without overriding the others by publishing them with a describing tag: $ docker image tag yt-dlp:alpine-3.19 &lt;username&gt;/yt-dlp:alpine-3.19 $ docker image push &lt;username&gt;/yt-dlp:alpine-3.19 $ docker image tag yt-dlp:python-alpine &lt;username&gt;/yt-dlp:python-alpine $ docker image push &lt;username&gt;/yt-dlp:python-alpine  Or if we don't want to keep the Ubuntu version anymore we can replace that pushing an Alpine-based image as the latest. Someone might depend on the image being Ubuntu though. $ docker image tag yt-dlp:python-alpine &lt;username&gt;/yt-dlp $ docker image push &lt;username&gt;/yt-dlp  It's important to keep in mind that if not specified, the tag :latest simply refers to the most recent image that has been built and pushed, which can potentially contain any updates or changes. "},{"title":"Exercise 3.7​","type":1,"pageTitle":"Optimizing the image size","url":"/part-3/section-4#exercise-37","content":"Exercise 3.7 As you may have guessed, you shall now return to the frontend and backend from the previous exercise. Change the base image in FROM to something more suitable. To avoid the extra hassle, it is a good idea to use a pre-installed image for both Node.js and Golang. Both should have at least Alpine variants ready in DockerHub. Note that the frontend requires Node.js version 16 to work, so you must search for a bit older image. Make sure the application still works after the changes. Document the size before and after your changes. "},{"title":"Multi-stage builds​","type":1,"pageTitle":"Optimizing the image size","url":"/part-3/section-4#multi-stage-builds","content":"Multi-stage builds are useful when you need some tools just for the build but not for the execution of the image (that is for CMD or ENTRYPOINT). This is an easy way to reduce size in some cases. Let's create a website with Jekyll, build the site for production and serve the static files with Nginx. Start by creating the recipe for Jekyll to build the site. FROM ruby:3 WORKDIR /usr/app RUN gem install jekyll RUN jekyll new . RUN jekyll build  This creates a new Jekyll application and builds it. We are going to use Nginx to serve the site page but you can test how the site works if you add the following directive: CMD bundle exec jekyll serve --host 0.0.0.0  We could start thinking about optimizations at this point but instead, we're going to add a new FROM for Nginx, this is what the resulting image will be. Then we will copy the built static files from the Ruby image to our Nginx image: # the first stage needs to be given a name FROM ruby:3 AS build-stage WORKDIR /usr/app RUN gem install jekyll RUN jekyll new . RUN jekyll build # we will now add a new stage FROM nginx:1.19-alpine COPY --from=build-stage /usr/app/_site/ /usr/share/nginx/html  Now Docker copies contents from the first image /usr/app/_site/ to /usr/share/nginx/html Note the naming from Ruby to build-stage. We could also use an external image as a stage, --from=python:3.12 for example. Let's build and check the size difference: $ docker build . -t jekyll $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE jekyll nginx 9e2f597ad99e 8 seconds ago 21.3MB jekyll ruby 5dae3d9f8dfb 26 minutes ago 1.05GB  As you can see, even though our Jekyll image needed Ruby during the build stage, it is considerably smaller since it only has Nginx and the static files in the resulting image. docker run -it -p 8080:80 jekyll:nginx also works as expected. Often the best choice is to use a FROM scratch image as it doesn't have anything we don't explicitly add there, making it the most secure option over time. "},{"title":"Exercises 3.8 - 3.10​","type":1,"pageTitle":"Optimizing the image size","url":"/part-3/section-4#exercises-38---310","content":"Exercise 3.8: Multi-stage frontend Do now a multi-stage build for the examplefrontend. Even though multi-stage builds are designed mostly for binaries in mind, we can leverage the benefits with our frontend project as having original source code with the final assets makes little sense. Build it with the instructions in README and the built assets should be in build folder. You can still use the serve to serve the static files or try out something else. Exercise 3.9: Multi-stage backend Let us do a multi-stage build for the backend project since we've come so far with the application. The project is in Golang and building a binary that runs in a container, while straightforward, isn't exactly trivial. Use resources that you have available (Google, example projects) to build the binary and run it inside a container that uses FROM scratch. To successfully complete the exercise the image must be smaller than 25MB. Exercise 3.10 Do all or most of the optimizations from security to size for one other Dockerfile you have access to, in your own project or for example the ones used in previous &quot;standalone&quot; exercises. Please document Dockerfiles both before and after. "}]